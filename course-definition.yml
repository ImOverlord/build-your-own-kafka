slug: "kafka"
name: "Build your own Kafka"
short_name: "Kafka"
release_status: "alpha"

description_md: |-
  Apache Kafka is a distributed event streaming platform often used for high-performance data pipelines. In this challenge, you'll build your own Kafka broker
  that's capable of serving basic requests.

  Along the way you'll learn about TCP servers, the Kafka wire protocol and more.

short_description_md: |-
  Learn about TCP servers, the Kafka wire protocol and more.

completion_percentage: 15

languages:
  - slug: "go"
  - slug: "python"
  - slug: "rust"

marketing:
  difficulty: medium
  sample_extension_idea_title: "On Disk Storage"
  sample_extension_idea_description: "A Kafka broker that can read and write to disk"
  testimonials:
    - author_name: "Ananthalakshmi Sankar"
      author_description: "Automation Engineer at Apple"
      author_avatar: "https://codecrafters.io/images/external/testimonials/oxta.jpeg"
      link: "https://github.com/anu294"
      text: "There are few sites I like as much that have a step by step guide. The real-time feedback is so good, it's creepy!"

    - author_name: "Patrick Burris"
      author_description: "Senior Software Developer, CenturyLink"
      author_avatar: "https://codecrafters.io/images/external/testimonials/patrick-burris.jpeg"
      link: "https://github.com/Jumballaya"
      text: |-
        I think the instant feedback right there in the git push is really cool.
        Didn't even know that was possible!

stages:
  - slug: "vi6"
    name: "Bind to a port"
    difficulty: easy
    description_md: |-
      In this stage, you'll implement a TCP server that listens on port 9092.

      [TCP](https://en.wikipedia.org/wiki/Transmission_Control_Protocol) is the underlying protocol used by protocols like HTTP, SSH and others
      you're probably familiar with. Kafka clients & brokers use TCP to communicate with each other.

      Don't worry if you're unfamiliar with the TCP protocol, or what Kafka clients & brokers are. You'll learn more about this in the
      next stages.

      ### Tests

      The tester will execute your program like this:

      ```bash
      $ ./your_program.sh
      ```

      It'll then try to connect to your TCP server on port 9092. If the connection succeeds, you'll pass this stage.

      ### Notes

      - 9092 is the default port that Kafka uses.
      - If you already have a Kafka broker running on your machine and listening on port 9092, you'll see a "port already in use" error when running your code. Try stopping the existing Kafka broker and running your code again.

    marketing_md: |-
      In this stage, you'll start a TCP server on port 9092, which is the
      default port that Redis uses.

  - slug: "nv3"
    name: "Send Correlation ID"
    difficulty: easy
    description_md: |-
      In this stage, you'll respond to a request with a hardcoded correlation ID.

      ### Kafka wire protocol

      The [Kafka wire protocol](https://kafka.apache.org/protocol.html) is a binary protocol used by Kafka brokers and clients to communicate with each other.

      All communication over this protocol happens in request-response pairs. A client sends a request to the broker, and the broker responds with a response.

      You'll implement support for this protocol over the next few stages. We'll break down the protocol into smaller pieces and implement them one by one.

      ### Kafka requests

      Examples of some commonly used Kafka requests:

      - `APIVersions`, used to get the supported API versions by the broker
      - `Fetch`, used to get data from the broker
      - `Produce`, used to send data to the broker

      We'll learn about these in detail later, for now we'll focus on the protocol format itself.

      ### Request/Response format

      As described in [the docs](https://kafka.apache.org/protocol.html#protocol_common), each request and response in the Kafka wire protocol starts with 4 bytes that
      describe the length of the entire message (this includes the header & body), followed by the message. This is true for both requests and responses.

      The first 4 bytes are encoded as an int32 (4 bytes, big-endian / network byte order).

      For example, if the length of the message is 10, the first 4 bytes (in hexadecimal) will be `00 00 00 0a` (`0a` is the hexadecimal representation of 10).

      ### Response header format

      Each response to a Kafka request contains a header and a body. The header format is the same for all responses (irrespective of request type), but the body format varies by request type.

      The response header format is as follows:

      - `correlation_id`: An integer that uniquely identifies the request (the client sends this value in the request)
        - This is an "INT32" according to the Kafka wire protocol
        - It is a signed integer, encoded using 4 bytes in the response (big-endian / network byte order)
      - `tag_buffer`: An array of optional "tagged fields"
        - This can be ignored for now, they're [optional tagged fields](https://cwiki.apache.org/confluence/display/KAFKA/KIP-482%3A+The+Kafka+Protocol+should+Support+Optional+Tagged+Fields) used to introduce additional features over time.

      As mentioned in the previous section, the header is preceded by 4 bytes that describe the length of the entire message (header and body combined).

      ### Response body format

      The response body format varies by request type. We'll cover this in later stages.

      ### Tests

      The tester will execute your program like this:

      ```bash
      $ ./your_program.sh
      ```

      It'll then connect to your server on port 9092 and send a `APIVersions` request. You don't need to implement the logic to parse this request yet.

      The tester will wait to receive a response from your server. It will expect at least 8 bytes of data (4 bytes for the message length + 4 bytes for the correlation ID).

      The tester won't validate the first 4 bytes of your response (the "message length") in this stage. The tester will assert that the next 4 bytes are `00 00 00 07` (7, encoded
      as a 4 byte integer in big-endian).

      ### Notes

      - In this stage you're expected to hardcode the correlation ID as `7` in your response. We'll get to reading this value from the request in later stages.
      - Since the response format is currently incomplete, the first 4 bytes of your response (the "message length") will not be validated in this stage. We'll get to this in later stages.
      - You don't need to parse the request in this stage. We'll get to this in later stages.
    marketing_md: |-
      In this stage, you'll start implementing the ResponseHeader.

  - slug: "wa6"
    name: "Parse Correlation ID"
    difficulty: medium
    description_md: |-
      In this stage, you'll parse the correlation ID from a request and send a response with the same correlation ID.

      ### Request header format

      In the previous stage, we saw that the request starts with a 4 byte length field, followed by the request header, and then the request body.

      The format of the request header is as follows:

      | Field Name            | Field Type      | Description                               |
      |-----------------------|-----------------|-------------------------------------------|
      | `request_api_key`     | INT16           | The API key for the request               |
      | `request_api_version` | INT16           | The version of the API for the request    |
      | `correlation_id`      | INT32           | A unique identifier for the request       |
      | `client_id`           | NULLABLE_STRING | The client ID for the request             |
      | `tagged_fields`       | TAGGED_FIELDS   | Optional tagged fields                    |

      The documentation for this can be found [here](https://kafka.apache.org/protocol.html#protocol_messages) under the "Request Header v3" section.

      - `request_api_key`: An integer identifying the type of request.
        - The value of this field is different for each type of request.
        - For example, the `APIVersions` request has a `request_api_key` of `18`.
        - A full list of API keys can be found in [the docs](https://kafka.apache.org/protocol.html#protocol_api_keys).
      - `request_api_version`: The version of the API for the request.
        - This is an integer that identifies the version of the API for the request.
        - For example, the `APIVersions` request supports multiple versions: `0`, `1`, `2` and `3`.
      - `correlation_id`: A unique identifier for the request.
        - This value is echo-ed back in the response.
        - (This is the value you hardcoded in the previous stage!)
      - `client_id`: A string identifying the client that sent the request.
      - `tagged_fields`: Optional tagged fields
        - This can be ignored for now, they're [optional tagged fields](https://cwiki.apache.org/confluence/display/KAFKA/KIP-482%3A+The+Kafka+Protocol+should+Support+Optional+Tagged+Fields) used to introduce additional features over time.

      In this stage, you'll only need to parse the `correlation_id` field. You can ignore the other fields for now.

      ### Tests

      The tester will execute your program like this:

      ```bash
      $ ./your_program.sh
      ```

      It'll then connect to your server on port 9092 and send a `APIVersions` request. The tester will include a random
      correlation ID in this request.

      The tester will wait for your program to respond and it'll then validate that the correlation ID in the response header matches the correlation ID it send in the request header.

      Just like the previous stage, the tester won't validate the first 4 bytes of your response (the "message length") in this stage.

      ### Notes

      - You can remove the hardcoded correlation ID of `7` we used in the previous stage. You can now read this value from the request.
      - Since the response format is currently incomplete, the first 4 bytes of your response (the "message length") will not be validated in this stage. We'll get to this in later stages.
      - You don't need to parse all fields in the request in this stage, just the `correlation_id` field. We'll get to the other fields in later stages.
    marketing_md: |-
      In this stage, you'll start decoding the RequestHeader.

  - slug: "nc5"
    name: "Parse API Version"
    difficulty: medium
    description_md: |-
      In this stage, you'll continue decoding the RequestHeader.

      ### Tests

      The tester will execute your program like this:

      ```bash
      $ ./your_program.sh
      ```

      It'll then connect to your server on port 9092 and send a `APIVersions:V3` request with an invalid `APIVersion` of -1.
      The tester will validate that the correlation ID in the response header matches
      the correlation ID in the request header. The tester will also validate that the error code in the response body is 35.
      This is the error code for `UNSUPPORTED_VERSION`.
      We are intentionally triggering this error to test that your server is correctly populating the error code in the response body.

      Although your server's response will need to contain the "message length" field (an int32), the tester will not verify this value in this stage.

      ### Notes
      - We will send an `APIVersions` request with an `APIVersion` of -1.

    marketing_md: |-
      In this stage, you'll start encoding your response to the `APIVersions` requests.

  - slug: "pv1"
    name: "Handle APIVersions requests"
    difficulty: hard
    description_md: |-
      In this stage, you'll implement the ResponseBody for the `APIVersions` request.

      ### Tests

      The tester will execute your program like this:

      ```bash
      $ ./your_program.sh
      ```

      It'll then connect to your server on port 9092 and send a valid `APIVersions:V3` request.
      The tester will validate that the correlation ID in the response header matches the correlation ID in the request header.
      The tester will also validate that the error code in the response body is 0.
      Then, it will validate that the response body contains atleast one APIVersionsKey response, for the APIKey `18` (API_VERSIONS).
      And that the `MaxVersion` for the `ApiKey` `18` is atleast `3`.

      ### Notes
      - From this stage onwards, we expect you to send well formed responses. The total response length should be prefixed to your response, and we will only read that many bytes.
      - If extra bytes are remaining after decoding all the fields of the response body, it will be an error.
      - You can expect to receive V3 of the APIVersions request, and also send V3 of the APIVersions response.

    marketing_md: |-
      In this stage, you'll need to implement the `APIVersions:V3` response.

  - slug: "gs0"
    name: "Include Fetch in APIVersions"
    difficulty: medium
    description_md: |-
      In this stage, you'll add Fetch to the APIVersions response.

      ### Tests

      The tester will execute your program like this:

      ```bash
      $ ./your_program.sh
      ```

      It'll then connect to your server on port 9092 and send a valid `APIVersions:V3` request.
      The tester will validate that the correlation ID in the response header matches the correlation ID in the request header.
      The tester will also validate that the error code in the response body is 0.
      Then, it will validate that the response body contains atleast one APIVersionsKey response, for the APIKey `1` (FETCH).
      And that the `MaxVersion` for the `ApiKey` `1` is atleast `16`.

      ### Notes
      - You can expect to receive V3 of the APIVersions request, and also send V3 of the APIVersions response.

    marketing_md: |-
      In this stage, you'll add the Fetch API to the APIVersions response.

  - slug: "dh6"
    name: "Fetch with an empty topic"
    difficulty: medium
    description_md: |-
      In this stage, you'll implement the Fetch response for a topic with 0 messages.

      ### Tests

      The tester will execute your program like this:

      ```bash
      $ ./your_program.sh
      ```

      It'll then connect to your server on port 9092 and send a `Fetch:V16` request. The tester will validate that the correlation ID in the response header matches
      the correlation ID in the request header. The tester will also validate that the error code in the response body is 0.
      Then, it will validate that the response body contains no topicResponses. (But as the response body contains COMPACT_ARRAY of topicResponses, the length should be prefixed as 1).
      The hexdump of a sample kafka response for this request is given below.

      ```
      0000   00 00 00 11 5e 33 46 cc 00 00 00 00 00 00 00 0a   ....^3F.........
      0010   b9 c1 2e 01 00                                    .....
      ```

      ### Notes
      - We will send an `Fetch` request with an `APIVersion` of 16.

    marketing_md: |-
      In this stage, you'll start encoding your response to the `Fetch` requests.

  - slug: "cm4"
    name: "Fetch with a hardcoded message"
    difficulty: hard
    description_md: |-
      In this stage, you'll implement the Fetch response for a topic with multiple messages (hardcoded).

      ### Tests

      The tester will execute your program like this:

      ```bash
      $ ./your_program.sh
      ```

      It'll then connect to your server on port 9092 and send a `Fetch:V16` request. The tester will validate that the correlation ID in the response header matches
      the correlation ID in the request header. The tester will also validate that the error code in the response body is 0.
      Then, it will validate that the response body contains a topicResponse for the topic provided in the request.
      And that the topicResponse contains a partitionResponse for the partition provided in the request.

      The partitionResponse should contain a messageSet with a RecordBatch.
      Which should in turn contain a Record which should contain a string with the hardcoded message.

      You need to send 3 such messages in the response.
      The messages should be `m1, m2, m3`.

      The RecordBatch has a CRC field, which is computed using the Castagnoli polynomial. This CRC field in the response should match with the CRC computed from the RecordBatch contents.

      The hexdump of a sample kafka response for this request is given below.

      ```
      0000   00 00 00 df b4 5e ed c3 00 00 00 00 00 00 00 1d   .....^..........
      0010   0d 45 6f 02 7d 98 b8 a8 4a 42 4e c8 a4 fa bc e4   .Eo.}...JBN.....
      0020   c9 5d 18 a6 02 00 00 00 00 00 00 00 00 00 00 00   .]..............
      0030   00 00 03 00 00 00 00 00 00 00 03 00 00 00 00 00   ................
      0040   00 00 00 00 ff ff ff ff 97 01 00 00 00 00 00 00   ................
      0050   00 00 00 00 00 44 00 00 00 00 02 59 1e f5 8f 00   .....D.....Y....
      0060   00 00 00 00 01 00 00 01 91 b2 3a 23 db 00 00 01   ..........:#....
      0070   91 b2 3a 25 f8 00 00 00 00 00 00 00 00 00 00 00   ..:%............
      0080   00 00 00 00 00 00 02 10 00 00 00 01 04 6d 31 00   .............m1.
      0090   12 00 ba 08 02 01 04 6d 32 00 00 00 00 00 00 00   .......m2.......
      00a0   00 02 00 00 00 3a 00 00 00 00 02 ce df 14 77 00   .....:........w.
      00b0   00 00 00 00 00 00 00 01 91 b2 3a 29 cc 00 00 01   ..........:)....
      00c0   91 b2 3a 29 cc 00 00 00 00 00 00 00 00 00 00 00   ..:)............
      00d0   00 00 02 00 00 00 01 10 00 00 00 01 04 6d 33 00   .............m3.
      00e0   00 00 00                                          ...
      ```

      ### Notes
      - We will send an `Fetch` request with an `APIVersion` of 16.

    marketing_md: |-
      In this stage, you'll implement the Fetch response for a topic with multiple messages.
